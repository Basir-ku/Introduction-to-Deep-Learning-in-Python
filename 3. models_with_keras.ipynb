{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9a6538-d295-454f-8889-f68814203b6e",
   "metadata": {},
   "source": [
    "# Building deep learning models with keras\n",
    "In this notebook, you'll use the Keras library to build deep learning models for both regression and classification. You'll learn about the Specify-Compile-Fit workflow that you can use to make predictions, and by the end of this, you'll have all the tools necessary to build deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6e474-5fc0-47cf-9f99-da098215db8a",
   "metadata": {},
   "source": [
    "#### Model building steps\n",
    "The Keras workflow has 4 steps. \n",
    "1. you specify the architecture, which is things like: how many layers do you want? how many nodes in each layer? What activation function do you want to use in each layer? \n",
    "\n",
    "2. you compile the model. This specifies the loss function, and some details about how optimization works. \n",
    "\n",
    "3. you fit the model. Which is that cycle of back-propagation and optimization of model weights with your data. \n",
    "\n",
    "4. finally you will want to use your model to make predictions. We'll go through these steps sequentially. The first step is creating or specifying your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76c7f8-9437-4c11-aa51-7dceeb077ff3",
   "metadata": {},
   "source": [
    "####  Model specification ( Regression model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c527f91-7480-48b4-8a82-9912aedb24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv('hourly_wages.csv')\n",
    "\n",
    "# Separate predictors and target\n",
    "predictors = df.drop('wage_per_hour', axis=1).values\n",
    "target = df['wage_per_hour'].values\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation=\"relu\", input_shape=(n_cols, )))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470844df-20b0-49f2-aef7-d621a5dfd62f",
   "metadata": {},
   "source": [
    "First we import what we will need. pandas is here only for reading some data. The other two imports are used for building our model. Then we seperate predictors and target from the dataset.\n",
    "We always need to specify how many columns (n_cols) are in the input when building a Keras model, because that is the number of nodes in the input layer. \n",
    "\n",
    "We then start building the model. The first line of model specification is model equals Sequential. There are two ways to build up a model, and we will focus on sequential, which is the easier way to build a model. Sequential models require that each layer has weights or connections only to the one layer coming directly after it in the network diagram. There are more exotic models out there with complex patterns of connections, but Sequential will do the trick for everything we need here. We start adding layers using the add method of the model. \n",
    "\n",
    "The type of layer you have seen, that standard layer type, is called a Dense layer. It is called Dense because all of the nodes in the previous layer connect to all of the nodes in the current layer. As you advance in deep learning, you may start using layers that aren't Dense. In each layer, we specify the number of nodes as the first positional argument, and the activation function we want to use in that layer using the keyword argument activation. Keras supports every activation function you will want in practice. In the first layer, we need to specify input shapes as shown here. That says the input will have n_cols columns, and there is nothing after the comma, meaning it can have any number of rows, that is, any number of data points. \n",
    "\n",
    "You'll notice the last layer has 1 node. That is the output layer. This model has 2 hidden layers, and an output layer. You may be struck that hidden layers has 50 and 32 nodes. Keras and TensorFlow do the math for us, so don't feel afraid to use much bigger networks than we've seen before. It's quite common to use 100 or 1000s nodes in a layer. You'll learn more about choosing an appropriate number of nodes later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c0fbc-678f-4083-82ce-02292538d5c3",
   "metadata": {},
   "source": [
    "#### Compiling a model\n",
    "\n",
    "After you've specified a model, the next task is to compile it, which sets up the network for optimization, for instance creating an internal function to do back-propagation efficiently.The compile methods has two important arguments for you to choose. \n",
    "\n",
    "The first is what optimizer to use, which controls the learning rate. In practice, the right choice of learning rate can make a big difference for how quickly our model finds good weights, and even how good a set of weights it can find. There are a few algorithms that automatically tune the learning rate. Even many experts in the field don't know all the details of all the optimization algorithms. So the pragmatic approach is to choose a versatile algorithm and use that for most problems. Adam is an excellent choice as your go-to optimizer. Adam adjusts the learning rate as it does gradient descent, to ensure reasonable values throughout the weight optimization process. \n",
    "\n",
    "The second thing you specify is the loss function. Mean squared error is the most common choice for regression problems. When we use Keras for classification, you will learn a new default metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84899ea-6ccc-4ed1-89ac-ca90dec7652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0ff11-de88-4f04-a405-c2a513c7d6df",
   "metadata": {},
   "source": [
    "#### fit a model\n",
    "After compiling the model, you can fit it. That is applying back-propagation and gradient descent with your data to update the weights. The fit step looks similar to what you've seen in scikit-learn, though it has more options which we will explore soon. Even with the Adam optimizer, which is pretty smart, it can improve your optimization process if you scale all the data so each feature is, on average, about similar sized values. One common approach is to subtract each feature by that features mean, and divide it by it's standard deviation.\n",
    "\n",
    "After the compile step, we run fit, with the predictors as the first argument. When you run this, you will see some output showing the optimizations progress as it fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d42a84-0b63-402b-b9fc-db9ff94b57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d570f5-3729-4b0f-ba58-519edbe1d034",
   "metadata": {},
   "source": [
    "#### Classification Models\n",
    "So far we have focused on regression models. But deep learning works similarly for classification, that is for predicting outcomes from a set of discrete options.\n",
    "\n",
    "For classification, you do a couple of things differently. The biggest changes are: first, you set the loss function as 'categorical_crossentropy' instead of 'mean_squared_error'. This isn't the only possible loss function for classification problems, but it is by far the most common. You may have heard of this before under the name LogLoss. We won't go into the mathematical details of categorical crossentropy here. For categorical crossentropy loss function, a lower score is better. But it's still hard to interpret. So I've added this argument \"metrics equals accuracy\". This means I want to print out the accuracy score at the end of each epoch, which makes it easier to see and understand the models progress.\n",
    "\n",
    "The second thing we do is you need to modify the last layer, so it has a separate node for each potential outcome. You will also change the activation function to softmax. The softmax activation function ensures the predictions sum to 1, so they can be interpreted as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17516a86-314c-46b6-8990-e23aa094a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('titanic_all_numeric.csv')\n",
    "\n",
    "# Separate predictors and target\n",
    "predictors = df.drop('survived', axis=1).values.astype('float32')\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "targets = to_categorical(df.survived)\n",
    "\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_clos = predictors.shape[1]\n",
    "\n",
    "# set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layers\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_clos, )))\n",
    "\n",
    "# Add the output layers\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, targets, validation_split=0.3, epochs=20, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8479fc-4385-40d4-badc-0af44b0d292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(pred_data)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:, 1]\n",
    "\n",
    "# Print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3f447-6c30-4bc8-8a2f-23fb1f9eb28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of the mddel\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffcf4a-79fd-4d33-87a4-79acd3fa0eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
