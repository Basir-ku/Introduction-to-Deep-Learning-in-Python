{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba28d2d5-93bc-4f06-a0f8-4509bcfb4f52",
   "metadata": {},
   "source": [
    "### Topics:\n",
    "Learn how to optimize your deep learning models in Keras. Start by learning how to validate your models, then understand the concept of model capacity, and finally, experiment with wider and deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788147e0-ab19-4550-8de4-a19d5349fbaa",
   "metadata": {},
   "source": [
    "### Understanding model optimization & Stochastic Gradient Descent\n",
    "\n",
    "In practice, optimization is a hard problem. The optimal value for any one weight depends on the values of the other weights, and we are optimizing many weights at once. Even if the slope tells us which weights to increase, and which to decrease, our updates may not improve our model meaningfully. A small learning rate might cause us to make such small updates to the model's weights that our model doesn't improve materially. A very large learning rate might take us too far in the direction that seemed good. A smart optimizer like Adam helps, but optimization problems can still occur. The easiest way to see the effect of different learning rates is to use the simplest optimizer, Stochastic Gradient Descent, sometimes abbreviated to SGD. This optimizer uses a fixed learning rate. Learning rates around point-01 are common. But you can specify the learning rate you need with lr argument. We create models in a for loop, and each time around we compile the model using SGD with a different learning rate. We pass in the optimizer with the same argument where we previously passed the string for \"Adam\". In an exercise, you will compare the results of training models trained with low, medium and high learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49360a11-ac3c-4be0-b0fe-659e342cf552",
   "metadata": {},
   "source": [
    "### The dying neuron problem & Vanishing gradients\n",
    "\n",
    "Even if your learning rate is well tuned, you can run into the so-called \"dying-neuron\" problem. This problem occurs when a neuron takes a value less than 0 for all rows of your data. With the ReLU activation function, any node with a negative input value produces an output of 0, and it also has a slope of 0. Because the slope is 0, the slope of any weights flowing into that node are also 0. So those weights don't get updated. In other words, once the node starts always getting negative inputs, it may continue only getting negative inputs. It's contributing nothing to the model at this point, and hence the claim that the node or neuron is \"dead.\"\n",
    "\n",
    "At first, this might suggest using an activation function whose slope is never exactly zero for example, an s-shaped function called tanh. However, values that were outside the middle of the S were relatively flat, or had small slopes. A small but non-zero slope might work in a network with only a few hidden layers. But in a deep network, one with many layers, the repeated multiplication of small slopes causes the slopes to get close to 0, which meant updates in backprop were close to 0. This is called the vanishing gradient problem. This in turn might suggest using an activation function that isn't even close to flat anywhere. There is research in this area, including variations on ReLU. Those aren't widely used though. For now, it's a phenomenon worth keeping in mind if you are ever pondering why your model isn't training better. If it happens, changing the activation function may be the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bff903-a3cd-46fc-861d-f1505316fd03",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "As you know, your model's performance on the training data is not a good indication of how it will perform on new data. For this reason, we use validation data to test model performance. Validation data is data that is explicitly held out from training, and used only to test model performance. \n",
    "\n",
    "You may already be familiar with k-fold cross validation. In practice, few people run k-fold cross validation on deep learning models because deep learning is typically used on large datasets. So the computational expense of running k-fold validation would be large, and we usually trust a score from a single validation run because those validation runs are reasonably large. Keras makes it easy to use some of your data as validation data, we specify the split using the keyword argument validation_split when calling the fit method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad66cf7-e920-4797-8ab9-84d07195845f",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "we should keep training while validation score is improving, and then stop training when the validation score isn't improving. We do this with something called \"early stopping.\". it takes an argument called patience, which is how many epochs the model can go without improving before we stop training. 2 or 3 are reasonable values for patience. Sometimes you'll get a single epoch with no improvement, but the model will start improving again after that epoch. But if you see 3 epochs with no improvement, it's unlikely to turn around and start improving again. We pass it to the fit function under an argument called callbacks. Note that callbacks takes a list. You may consider adding other callbacks as you become very advanced. But early stopping is all you want for now. \n",
    "\n",
    "By default, keras trains for 10 epochs. Now that we have smart logic for determining when to stop, we can set a high maximum number of epochs. This happens with the epochs argument. Keras will go until this number of epochs, unless the validation loss stops improving, in which case it will stop earlier. This is smarter training logic than relying on a fixed number of epochs without looking at the validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af64023-7493-4622-8c6c-fd77ea8ef13a",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "To experiment with different architectures. More layers, fewer layers. Layers with more nodes, layers with fewer nodes. And so on. Creating a great model requires some experimentation. Before we finish, I'll give a little bit of insight into how to choose where you experiment. But, now that you can get validation scores, you are poised to run those experiments and figure out what works best for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1ef72-6ecd-4cd7-9fbf-e937f6a460b2",
   "metadata": {},
   "source": [
    "### Changing optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd87bcd-eb54-4bcc-b853-20fe424d43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SGD optimizer\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer, loss=\"categorical_crossentropy\")\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b753a-7339-4d30-967d-882acd4efd04",
   "metadata": {},
   "source": [
    "### Evaluating model accuracy on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dce872-c7e9-4095-977b-6d7651fd0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(predictors, target, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815ce37-1919-44b2-b33c-cb0e8a5782ad",
   "metadata": {},
   "source": [
    "#### Early stopping: Optimizing the optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8432359-d5b0-48d3-bcbe-bace738f2dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target,epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e36b22-15cd-472e-967d-68f2480d1889",
   "metadata": {},
   "source": [
    "#### Experimenting with wider networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947de9e1-ae55-4cce-8d5d-34deb0b872dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d51e5f-6a85-4762-aae2-fb41ba92d13a",
   "metadata": {},
   "source": [
    "### Adding layers to a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff2fe5-a141-4477-af49-33c452cc5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first, second, and third hidden layers\n",
    "model_2.add(Dense(10, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(10, activation='relu'))\n",
    "model_2.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5dd7ff-45ed-4b47-b59c-a7fb10aa8179",
   "metadata": {},
   "source": [
    "### Thinking about model capacity\n",
    "At this point, you know how to run experiments, and compare different models performance. However, it takes some practice to get an intuition for what experiments or architectures to try. There is still a little more art to finding good deep learning architectures than there is for tuning other machine learning algorithms. But something called \"model capacity\" should be one of the key considerations you think about when deciding what models to try.\"Model capacity\" or \"network capacity\" is closely related to the terms overfitting and underfitting.\n",
    "\n",
    "Overfitting is the ability of a model to fit oddities in your training data that are there purely due to happenstance, and that won't apply in a new dataset. When you are overfitting, your model will make accurate predictions on training data, but it will make inaccurate predictions on validation data and new datasets. Underfitting is the opposite. That is when your model fails to find important predictive patterns in the training data. So it is accurate in neither the training data nor validation data. Because we want to do well on new datasets that weren't used for training the model, our validation score is the ultimate measure of a model's predictive quality. \n",
    "\n",
    "Let's get back to model capacity. Model capacity is a model's ability to capture predictive patterns in your data. So, the more capacity a model, leads to overfitting. So, with that in mind, here is a good workflow for you. Start with a simple network, and get the validation score. Then keep adding capacity as long as the score keeps improving. Once it stops improving, you can decrease capacity slightly, but you are probably near the ideal.\n",
    "\n",
    "You can experiment. But you should generally be thinking about whether you are trying to increase or decrease capacity, ideally honing in on the right capacity by looking at validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ba9b5-dcac-48f3-ba4e-58f7568b0003",
   "metadata": {},
   "source": [
    "### Recognizing handwritten digits\n",
    "The MNIST dataset, which contains images of handwritten digits. This is a very popular dataset for getting started working with images. There is an image of each handwritten digit, and each image is composed of a 28 pixel by 28 pixel grid. The image is represented by showing how dark each pixel is. So, 0 would be as light as possible, and 255 is as dark as possible. I've flattened the 28 x 28 grid for you into a 784 x 1 array for each image. Each image shows a digit like 0, 1, 2, 3 4, all the way up to 9. Your model will predict which digit it is that was written. So you will create a deep learning model taking in those 784 features for each image as inputs, and predicting digits from among 10 possible values for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b89650-3060-42a7-b112-6a08ac17c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(784, )))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, validation_split=0.3, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
