{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2d9704-2720-4cbf-8a14-c07a7efb44f8",
   "metadata": {},
   "source": [
    "### Need for Optimization:\n",
    "You've seen the forward-propagation algorithm that neural networks use to make predictions. However, the mere fact that a model has the structure of a neural network does not guarantee that it will make good prediction. We need to optimize the model. Changing any weight will change our prediction. Let’s see what happens if we change the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d61595-afde-4943-95fa-0c4217c17c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights[\"node_0\"]).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights[\"node_1\"]).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights[\"output\"]).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dacd117-f984-4ad1-9c1f-1b1df455c047",
   "metadata": {},
   "source": [
    "### Coding how weight changes affect accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad90e22-9c86-4bf4-852a-b3105ca4c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "# The data point you will make a prediction for\n",
    "input_data = np.array([0, 3])\n",
    "\n",
    "# Sample weights\n",
    "weights_0 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "# The actual target value, used to calculate the error\n",
    "target_actual = 3\n",
    "\n",
    "# Make prediction using original weights\n",
    "model_output_0 = predict_with_network(input_data, weights_0)\n",
    "\n",
    "# Calculate error: error_0\n",
    "error_0 = model_output_0 - target_actual\n",
    "\n",
    "# Create weights that cause the network to make perfect prediction (3): weights_1\n",
    "weights_1 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [-1, 1]\n",
    "            }\n",
    "\n",
    "# Make prediction using new weights: model_output_1\n",
    "model_output_1 = predict_with_network(input_data, weights_1)\n",
    "\n",
    "# Calculate error: error_1\n",
    "error_1 = model_output_1 - target_actual\n",
    "\n",
    "# Print error_0 and error_1\n",
    "print(error_0)\n",
    "print(error_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f923ab4-9e48-4543-afde-7677d71e24db",
   "metadata": {},
   "source": [
    "So, this change in weights minimize the error, thus improved the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb266062-8983-460d-bc5b-af3928218437",
   "metadata": {},
   "source": [
    "#### Scaling up to multiple data points\n",
    "Making accurate predictions gets harder with multiple points. First of all, at any set of weights, we have many values of the error, corresponding to the many points we make predictions for. We use something called a loss function to aggregate all the errors into a single measure of the model's predictive performance. a common loss function for regression tasks is mean-squared error. You square each error, and take the average of that as a measure of model quality. The loss function aggregates all of the errors into a single score. Lower values mean a better model, so our goal is to find the weights giving the lowest value for the loss function. We do this with an algorithm called gradient descent. An analogy may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd2446f-512d-4873-9184-4de66802f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error with weights_0: 37.500000\n",
      "Mean squared error with weights_1: 49.890625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\n",
    "\n",
    "target_actuals = [1, 3, 5, 7]\n",
    "\n",
    "weights_0 = {'node_0': np.array([2, 1]), 'node_1': np.array([1, 2]), 'output': np.array([1, 1])}\n",
    "weights_1 = {'node_0': np.array([2, 1]), 'node_1': np.array([1. , 1.5]), 'output': np.array([1. , 1.5])}\n",
    "\n",
    "# Create model_output_0 \n",
    "model_output_0 = []\n",
    "# Create model_output_1\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network(row, weights_0))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network(row, weights_1))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e23ac-68c1-45b7-8a50-16f315a4ecc7",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "Imagine you are in a pitch dark field, and you want to find the lowest point. You might feel the ground to see how it slopes, and take a small step downhill. This gives an improvement, but not necessarily the lowest point yet. So you repeat this process until it is uphill in every direction. This is roughly how gradient descent works. The steps are: Start at a random point, until you are somewhere flat, find the slope, and take a step downhill.\n",
    "\n",
    "With gradient descent, you repeatedly repeatedly found a slope capturing how your loss function changes as a weight changes. You then made a small change to the weight to get to a lower point, and you repeated this until you couldn't go downhill any more If the slope is positive, going opposite the slope means moving to lower numbers. Subtracting the slope from the current value achieves this. But too big a step might lead us far astray. So, instead of directly subtracting the slope, we multiply the slope by a small number, called the learning rate, and we change the weight by the product of that multiplication. Learning rate are frequently around point-01. This ensures we take small steps, so we reliably move towards the optimal weights. But how do we find the relevant slope for each weight we need to update? Working this out for yourself involves calculus, especially the application of the chain rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae9161-7c50-4cd8-a87e-8bae62a87147",
   "metadata": {},
   "source": [
    "#### Calculating slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79bcc04-d3a1-4bfa-b1e1-0d7fd66115a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 28 42]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0, 2, 1])\n",
    "input_data = np.array([1, 2, 3])\n",
    "target = 0\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = input_data * error * 2\n",
    "\n",
    "# Print the slope\n",
    "print(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63660bd4-5b3e-4b4f-91c6-d4efea29c1e0",
   "metadata": {},
   "source": [
    "#### Improving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e45f121-66e9-47e3-b7ad-f303766e9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - (learning_rate * slope)\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d817219f-d6db-44c7-9aea-44541b9d2b11",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "You’ve used gradient descent to optimize weights in a simple model. Now we'll add a technique called “back propagation” to calculate the slopes you need to optimize more complex deep learning models.\n",
    "\n",
    "Just as forward propagation sends input data through the hidden layers and into the output layer, back propagation takes the error from the output layer and propagates it backward through the hidden layers, towards the input layer. \n",
    "\n",
    "It calculates the necessary slopes sequentially from the weights closest to the prediction, through the hidden layers, eventually back to the weights coming from the inputs. We then use these slopes to update our weights as you've seen. Back propagation is tricky. So you should focus on the general structure of the algorithm, rather than trying to memorize every mathematical detail.\n",
    "\n",
    "In the big picture, we are trying to estimate the slope of the loss function with respect to each weight in our network. You've already seen that we use prediction errors to calculate some of those slopes. So we always do forward propagation to make a prediction and calculate an error before we do back propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fa1f1-714b-4e03-8a95-3ecbe76716f1",
   "metadata": {},
   "source": [
    "#### ReLU function:\n",
    "For the Relu function the slope is 0 if the input into a node is negative. If the input into the node is positive, the output is the same as the input. So the slope would be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a68d3-2391-4ffc-8f2f-b3e90376a68e",
   "metadata": {},
   "source": [
    " #### Backpropagation: Recap\n",
    " \n",
    "we start at some random set of weights. We then go through the following iterative process Use forward propagation to make a prediction. Use backward propagation to calculate the slope of the loss function with respect to each weight. Multiply that slope by the learning rate, and subtract that from the current weights. Keep going with that cycle until we get to a flat part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a057931-59d9-464f-8988-2d637d24d4e5",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent\n",
    "For computational efficiency, it is common to calculate slopes on only a subset of the data, called a batch, for each update of the weights. You then use a different batch of data to calculate the next update. Once we have used all our data, we start over again at the beginning of the data. Each time through the full training data is called an epoch. So if we're going through our data for the 3rd time, we'd say we are on the 3rd epoch. When slopes are calculated on one batch at a time, rather than on the full data, that is called stochastic gradient descent, rather than gradient descent, which uses all of the data for each slope calculation. The process will be partially automated for you, but understanding the process will help fix any surprises that come up when building your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275663a-4b77-4948-8bfd-1224fa189a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
